# 3. Clustering and classification

The topics of this chapter - clustering and classification - are handy and **visual tools** of **exploring statistical data**. Clustering means that some points (or observations) of the data are in some sense **closer** to each other than some other points. In other words, the data points do not comprise a homogeneous sample, but instead, it is somehow clustered.

In general, the clustering methods try to **find these clusters** (or groups) from the data. One of the most typical clustering methods is called **k-means clustering**. Also **hierarchical clustering** methods quite popular, giving tree-like **dendrograms** as their main output.

As such, clusters are easy to find, but what might be the "right" number of clusters? It is not always clear. And how to give these clusters names and interpretations?

Based on a successful clustering, we may try to **classify** new observations to these clusters and hence validate the results of clustering. Another way is to use various forms of **discriminant analysis**, which operates with the (now) known clusters, asking: "what makes the difference(s) between these groups (clusters)?"

In the connection of these methods, we also discuss the topic of **distance** (or **dissimilarity** or **similarity**) measures. There are lots of other measures than just the ordinary Euclidean distance, although it is one of the most important ones. Several discrete and even binary measures exist and are widely used for different purposes in various disciplines.

### Data set information

The Boston dataset is one of the included dataset in the MASS library. The dataset describes housing values in suburbs of Boston. The following variables exist in the dataset, with **medv** being the target variable.

- **crim**: per capita crime rate by town.
- **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.
- **indus**: proportion of non-retail business acres per town.
- **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- **nox**: nitrogen oxides concentration (parts per 10 million).
- **rm**: average number of rooms per dwelling.
- **age**: proportion of owner-occupied units built prior to 1940.
- **dis**: weighted mean of distances to five Boston employment centres.
- **rad**: index of accessibility to radial highways.
- **tax**: full-value property-tax rate per $10,000.
- **ptratio**: pupil-teacher ratio by town.
- **black**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- **lstat**: lower status of the population (percent).
- **medv**: median value of owner-occupied homes in $1000s.

```{r, echo=FALSE, message=FALSE}
library(MASS)
data("Boston")
str(Boston)
```

### Graphical and Numerical Overview

Most of the variables have their own measuring units as mentioned in the previous section. **chas** is simply represented using a binary value (0 or 1)

```{r, echo=FALSE}
summary(Boston)
```

To ease the initial analysis, a correlation matrix is built and visualized as following. Housing value **medv** seems to negatively correlated with the proportion of lower class population **lstat**, pupil-teacher ratio **ptratio**, **tax**, as well as industrial areas **indus**. On the other hand, **medv** is positively correlated to the number of room s per dwelling **rm**

```{r, echo=FALSE, message=FALSE}
library(corrplot)
library(tidyverse)

# visualize the correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
```

To investigate what factors influence crime rate **crim**, let's take look at a slightly modified look of the same visualization. **crim** is positively increased as **tax** increased, as well as the accessibility to radial highways **rad**. The proportion of lower class population **lstat** is also correlated.

```{r, echo=FALSE}
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
```

### Standardizing the dataset

The data is scaled (normalized) and summarized as below. Since the value ranges are centered, all mean values are now 0. This way we can use the data for clustering methods, which utilize distance calculation methods, which require each feature to contribute approximately proportionately to the final distance.

```{r, echo=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Crime rate **crim** is replaced with categorical value **crime** (*low*, *med_low*, *med_high*, *high*) using the following code.

```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

The dataset is divided into two. 80% of the dataset will be used for training, while the remaining 20% will be used for testing.

```{r}
# number of rows in the Boston dataset 
n <- nrow(Boston)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

### Linear discriminant analysis

Linear discriminant analysis is performed on the training set with crime rate **crime** as the target variable and all the other variables in the dataset as predictor variables. A LDA biplot is visualized as below.

```{r, echo=FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)

lda.arrows(lda.fit, myscale = 2)
```

By performing a prediction using the trained linear discriminant model on the test dataset, we obtained a tabulated table on the crime rate **crime**. The diagonal line represents the correct predictions, which mostly account for the majorities. The incorrect number of predictions are kept relatively low.

```{r, echo=FALSE}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

### K-Means clustering

The Boston dataset is reloaded (and scaled). From here a typical Euclidean and Manhattan distance is calculated to give an idea about the similarity or dissimilarity of observations.

```{r, echo=FALSE}
# reload the scaled Boston dataset
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```

```{r}
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

# look at the summary of the distances
summary(dist_man)
```

K-Means already runs a built-in distance calculation for its algorithm. So here we run the K-Means algorithm on the Boston dataset with a random number of clusters being 3. However, there isn't a clear distinction between different class of crime rates with this config.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

To find the optimal number of classes (clusters), we visualize the change of total of within cluster sum of squares (WCSS) as k changes. The point of occurence where WCSS changes sharply is where the optimal k lies. In this case, it's roughly 2.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

We rerun the K-Means classification algorithm with the number of clusters found above (2). While not perfect, however, the crime rate **crim** classes are now better separated when evaluated against other variables.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```
