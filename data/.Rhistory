setwd("~/Workspace/hy/open-data-science/IODS-project/data")
setwd("~/Workspace/hy/open-data-science/IODS-project")
library(MASS)
data("Boston")
dim(Boston)
str(Boston)
library(MASS)
data("Boston")
str(Boston)
cor_matrix<-cor(Boston) %>% round(2)
library(corrplot)
install.packages(corrplot)
install.packages("corrplot")
library(corrplot)
library(tidyverse)
install.packages("tidyverse")
library(corrplot)
library(tidyverse)
cor_matrix<-cor(Boston) %>% round(2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
library(corrplot)
library(tidyverse)
# visualize the correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
library(corrplot)
library(tidyverse)
# visualize the correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "b", tl.cex = 0.8)
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "d", tl.pos = "d", tl.cex = 0.8)
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
summary(Boston)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
# number of rows in the Boston dataset
n <- nrow(Boston)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
# number of rows in the Boston dataset
n <- nrow(Boston)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# reload the scaled Boston dataset
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(Boston)
# look at the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")
# look at the summary of the distances
summary(dist_man)
# reload the scaled Boston dataset
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# look at the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")
# look at the summary of the distances
summary(dist_man)
# reload the scaled Boston dataset
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# look at the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")
# look at the summary of the distances
summary(dist_man)
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scaled[6:10], col = km$cluster)
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
setwd("~/Workspace/hy/open-data-science/IODS-project/data")
# 1. 2. Read in the “Human development” and “Gender inequality” datasets
hd <- read.csv(
"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv(
"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# 3. Explore the datasets
str(hd)
str(gii)
gii[2]
gii[2] %>% tail(10)
gii[3] %>% tail(10)
# 3. Explore the datasets
str(hd)
str(gii)
summary(hd)
# 4.
colnames(hd)[1]
# 4.
colnames(hd)[1] <- 'hdi_rank'
colnames(hd)[1]
# 3. Explore the datasets
# Structure and summaries of variables in "Human development" dataset
str(hd)
colnames(hd)[2] <- 'country'
colnames(hd)[3] <- 'hdi'
colnames(hd)[4] <- 'life_ex'
colnames(hd)[5] <- 'ex_edu_years'
colnames(hd)[6] <- 'mean_edu_years'
colnames(hd)[7] <- 'gni'
colnames(hd)[8] <- 'gni_rank'
str(gii)
colnames(gii)[1] <- 'gii_rank'
colnames(gii)[2] <- 'country'
colnames(gii)[3] <- 'gii'
colnames(gii)[4] <- 'maternal_death_rate'
colnames(gii)[5] <- 'teen_birth_rate'
colnames(gii)[6] <- 'per_rep_parliment'
colnames(gii)[7] <- 'second_edu_f'
colnames(gii)[8] <- 'second_edu_m'
colnames(gii)[9] <- 'labour_f'
colnames(gii)[10] <- 'labour_m'
str(gii)
# 5.
gii[second_edu_f_m_ratio] = gii$second_edu_f / gii$second_edu_m
# 5.
gii$second_edu_f_m_ratio = gii$second_edu_f / gii$second_edu_m
str(gii)
gii$labour_f_m_ratio = gii$labour_f / gii$labour_m
str(gii)
str(hd)
str(gii)
# 6. Joining hd and gii datasets
library(dplyr)
join_by <- c("country")
human <- inner_join(hd, gii, by = join_by)
str(human)
# Write data to file
write.table(human, file = "human.csv", sep = ";")
